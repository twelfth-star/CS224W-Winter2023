# Introduction

可信赖的 AI/GNN 包含很多方面：鲁棒性、可解释性、隐私、公平性、可说明性、环境福祉等。

对于 GNN，它的挑战在于图拓扑在这些问题中的作用是以前没有探索过的，定量评估通常很困难。

# Explainability and its Problem Settings

可解释的人工智能（XAI）：使用户能够理解模型的决策，并获得人类用户对深度学习系统的信任。

以下是一些 ML/DL 模型的可解释性例子： 

* 线性回归
	* 权重/斜率表示对应特征的重要性
* 降维
	* 边界处的实例描述了不同类的不同之处
* 决策树
	* 决策树具有高度可解释性！
	* 在决策树的每个节点上，我们都能了解到一个预测标准
* CV
	* 图像的特定区域显示了预测的对象类别
* NLP
	* 一些重要的 token 导致某种预测

让模型可解释的因素
* 重要的某些量
* 输入特征与预测之间的直接的关系
* 鼓励使用概念或原型去解释

在 GNN 中的可解释性：一个重要的子图结构和一个节点特征的小子集，在对某个节点 $v$ 的 GNN 预测中起着至关重要的作用。

# GNNExplainer

# Explainability Evaluation



